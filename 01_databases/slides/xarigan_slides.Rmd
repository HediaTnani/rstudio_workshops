---
title: "Working with Databases in R"
author: "Hadrien Dykiel & Alex Gold\n(Credit: Edgar Ruiz, James Blair)"
date: "2020-02-26"
output: xaringan::moon_reader
---

# I. 3 Strategies for Efficient Queries

## 1. Sample and Model

## 2. Chunk and Pull

## 3. Push Compute to Data

# II. The Mechanics

Several ways of connecting, several ways of interacting

- Connections Pane
- Code

- SQL code chunks (or scripts)
- send query
- dplyr

## Connecting to Databases

### 1.1 - Connect to a database

1. Click on the `Connections` tab

2. Click on the `New Connection` button

3. Select `Postgres Dev`

4. Click OK

### 1.2 - Explore the database using the RStudio IDE

1. Expand the `datawarehouse` schema

2. Expand the `airport` table

3. Click on the table icon to the right of the `airport` table 

4. *(Optional)* Expand and explore the other tables

5. Click on the *disconnect* icon to close the connection


### 1.3 - List drivers and DSNs 

1. To get a list of drivers available in the server
```{r, access}
library(odbc)
odbcListDrivers()[1:2]
```

2. Click on the *ellipsis* button located in the **Files** tab

3. Type: `/etc`

4. Locate and open the `odbcinst.ini` file

5. To see a list of DSNs available in the server
```{r}
odbcListDataSources()
```

6. Using the *ellipsis* button again, navigate to `/etc/odbc.ini`

### 1.4 - Connect to a database using code

1. Run the following code to connect
```{r}
library(DBI)
con <- dbConnect(odbc::odbc(), "Postgres Dev")
```

2. Use `dbListTables()` to retrieve a list of tables
```{r}
dbListTables(con)
```

3. Use `dbGetQuery()` to run a quick query
```{r}
odbc::dbGetQuery(con, "SELECT * FROM datawarehouse.airport LIMIT 10")
```

4. Use the SQL chunk
```{sql, connection = con}
SELECT * FROM datawarehouse.airport LIMIT 10
```

5. Use the `output.var` option to load results to a variable
```{sql, connection = con, output.var = "sql_top10"}
SELECT * FROM datawarehouse.airport LIMIT 10
```

6. Test the variable
```{r}
sql_top10
```

7. Disconnect from the database using `dbDisconnect()`
```{r}
dbDisconnect(con)
```

### 1.5 - RStudio SQL Script
*Try out the new SQL Script support in RStudio 1.2*

1. Open the *query-example.sql* file

2. Click the *Preview* button. It is located in the top-right area of the script

3. In the script, change *airport* to *carrier*

4. Click on *Preview* again

### 1.6 - Connect to a database without a DSN
*A more complex way of connecting to a database, using best practices: http://db.rstudio.com/best-practices/managing-credentials/#prompt-for-credentials *
1. Use the following code to start a new connection that does not use the pre-defined DSN
```{r, eval = FALSE}
con <- dbConnect(
  odbc::odbc(),
  Driver = "PostgreSQL",
  Server = "localhost",
  UID    = rstudioapi::askForPassword("Database user"),
  PWD    = rstudioapi::askForPassword("Database password"),
  Port = 5432,
  Database = "postgres"
)
```
2. When prompted, type in **rstudio_dev** for the user, and **dev_user** as the password

3. Disconnect from the database using `dbDisconnect()`
```{r}
dbDisconnect(con)
```

### 1.7 - Secure credentials in a file

1. Open and explore the `config.yml` file available in your working directory

2. Load the `datawarehouse-dev` values to a variable
```{r}
dw <- config::get("datawarehouse-dev")
```

3. Check that the variable loaded propery, by checking the `driver` value
```{r}
dw$driver
```
4. Use info in the config.yml file to connect to the database
```{r}
con <- dbConnect(odbc::odbc(),
   Driver = dw$driver,
   Server = dw$server,
   UID    = dw$uid,
   PWD    = dw$pwd,
   Port   = dw$port,
   Database = dw$database
)
```

5. Disconnect from the database using `dbDisconnect()`
```{r}
dbDisconnect(con)
```

### 1.8 - Environment variables

1. Open and explore the `.Renviron` file available in your working directory

2. Confirm that the environment variables are loaded by using `Sys.getenv()`
```{r}
Sys.getenv("uid")
```

3. Pass the credentials using the environment variables
```{r}
con <- dbConnect(
  odbc::odbc(),
  Driver = "PostgreSQL",
  Server = "localhost",
  UID    = Sys.getenv("uid"),
  PWD    = Sys.getenv("pwd"),
  Port = 5432,
  Database = "postgres"
)
```

4. Disconnect from the database using `dbDisconnect()`
```{r}
dbDisconnect(con)
```


## dplyr + Databases = dbplyr

### 2.1 - Create a table variable

1. Load the `dplyr`, `DBI` and `dbplyr` libraries
```{r, dplyr}
library(dplyr)
library(dbplyr)
library(DBI)
```

2. *(Optional)* Open a connection to the database if it's currently closed
```{r}
con <- dbConnect(odbc::odbc(), "Postgres Dev")
```

3. Use the `tbl()` and `in_schema()` functions to create a reference to a table
```{r}
tbl(con, in_schema("datawarehouse", "airport"))
```

4. Load the reference, not the table data, into a variable
```{r}
airports <- tbl(con, in_schema("datawarehouse", "airport"))
```

5. Call the variable to see preview the data in the table
```{r}
airports
```

6. Set up the pointers to the other of the tables
```{r}
flights <- tbl(con, in_schema("datawarehouse", "vflight"))
carriers <- tbl(con, in_schema("datawarehouse", "carrier"))
```

### 2.2 - Under the hood 

1. SQL statement that actually runs when we ran `airports` as a command
```{r}
show_query(airports)
```

2. Easily view the resulting query by adding `show_query()` in another piped command
```{r}
airports %>%
  show_query()
```

3. Insert `head()` in between the two statements to see how the SQL changes
```{r}
airports %>%
  head() %>%
  show_query()
```

4. Use `sql_render()` and `simulate_mssql()` to see how the SQL statement changes from vendor to vendor
```{r}
airports %>%
  head() %>%
  sql_render(con = simulate_mssql()) 
```

### 2.3 -  Un-translated R commands

1. Preview how `Sys.time()` is translated
```{r}
airports %>%
  mutate(today = Sys.time()) %>%
  show_query()
```

2. Use PostgreSQL's native commands, in this case `now()`
```{r}
airports %>%
  mutate(today = now()) %>%
  show_query()
```

3. Run the `dplyr` code to confirm it works
```{r}
airports %>%
  mutate(today = now()) %>%
  select(today) %>%
  head()
```

### 2.4 -Using bang-bang

1. Preview how `Sys.time()` is translated
```{r}
airports %>%
  mutate(today = Sys.time()) %>%
  show_query()
```

2. Preview how `Sys.time()` is translated when prefixing `!!`
```{r}
airports %>%
  mutate(today = !!Sys.time()) %>%
  show_query()
```

3. Preview how `Sys.time()` is translated when prefixing `!!`
```{r}
airports %>%
  mutate(today = !!Sys.time()) %>%
  select(today) %>%
  head()
```

### 2.5 - knitr SQL engine

1. Copy the result of the latest `show_query()` exercise
```{r}
airports %>%
  mutate(today = !!Sys.time()) %>%
  show_query()
```

2. Paste the result in this SQL chunk
```{sql, connection = con}
SELECT "airport", "airportname", "city", "state", "country", "lat", "long", '2018-01-26T14:50:10Z' AS "today"
FROM datawarehouse.airport
```

### 2.6 - Basic aggregation

1. How many records are in the **airport** table?
```{r}
tbl(con, in_schema("datawarehouse", "airport"))  %>%
  tally()
```

2. What is the average character length of the airport codes? How many characters is the longest and the shortest airport name?
```{r}
airports %>%
  summarise(
    avg_airport_length = mean(str_length(airport), na.rm = TRUE),
    max_airport_name = max(str_length(airportname), na.rm = TRUE),
    min_airport_name = min(str_length(airportname), na.rm = TRUE),
    total_records = n()
  )
```

3. How many records are in the **carrier** table?
```{r}
```

4. How many characters is the longest **carriername**?
```{r}
```

5. What is the SQL statement sent in exercise 4?
```{r}
```


# III. Deployment to RStudio Connect
### 11.1 - Publish dashboard

1. Open the dashboard `app.R` file

2. Click on File

3. Click on Publish

4. Connect Account click Next

5. Select RStudio Connect

6. Copy and paste **your** RStudio Server URL and add `/rsconnect`

7. Enter your credentials

8. Complete the form

9. Click Proceed

10. Click on Connect

11. Click Publish

### 11.2 - Schedule scoring

1. Create a new RMarkdown

2. Start the new RMarkdown by loading all the needed libraries, connecting to the DB and setting `table_flights`
```{r, eval = FALSE}
library(tidyverse)
library(dbplyr)
library(tidypredict)
library(DBI)
library(lubridate)
con <- DBI::dbConnect(odbc::odbc(), "Postgres Dev")
table_flights <- tbl(con, in_schema("datawarehouse", "flight"))
```

3. Read the parsed model saved in exercise 5.6
```{r}
parsedmodel <- yaml::read_yaml("my_model.yml")
```

4. Copy the code from exercise 5.5 step 4. Load the code into a variable called *predictions*.  Change the model variable to *parsedmodel*
```{r}
predictions <- table_flights %>%
  filter(month == 2,
         dayofmonth == 1) %>%
    mutate(
    season = case_when(
      month >= 3 & month <= 5  ~ "Spring",
      month >= 6 & month <= 8  ~ "Summmer",
      month >= 9 & month <= 11 ~ "Fall",
      month == 12 | month <= 2  ~ "Winter"
    )
  ) %>%
  select( season, depdelay) %>%
  tidypredict_to_column(parsedmodel) %>%
  remote_query()
```

5. Change the `select()` verb to include `flightid`, and rename to `p_flightid` 
```{r}
select(p_flightid = flightid, season, depdelay) %>%
```


6. Append to the end, the SQL code needed to run the update inside the database
```{r}
update_statement <- build_sql(
  "UPDATE datawarehouse.flight SET nasdelay = fit FROM (",
  predictions,
  ") as p ",
  "WHERE flightid = p_flightid",
  con = con
)
dbSendQuery(con, update_statement)
```

7. `knit` the document to confirm it works

8. Click on File and then Publish

9. Select *Publish just this document*.  Confirm that the `parsemodel.csv` file is included in the list of files that are to be published.

10. In RStudio Connect, select `Schedule`

11. Click on `Schedule output for default`

12. Click on `Run every weekday (Monday to Friday)`

13. Click Save

### 11.3 - Scheduled pipeline

1. Create a new **RMarkdown** document

2. Copy the code from the **Class catchup** section in Spark Pipeline, unit 8
```{r}
library(tidyverse)
library(sparklyr)
library(lubridate)
top_rows <- read.csv("/usr/share/flights/data/flight_2008_1.csv", nrows = 5)
file_columns <- top_rows %>%
  rename_all(tolower) %>%
  map(function(x) "character")
conf <- spark_config()
conf$`sparklyr.cores.local` <- 4
conf$`sparklyr.shell.driver-memory` <- "8G"
conf$spark.memory.fraction <- 0.9
sc <- spark_connect(master = "local", config = conf, version = "2.0.0")
spark_flights <- spark_read_csv(
  sc,
  name = "flights",
  path = "/usr/share/flights/data/",
  memory = FALSE,
  columns = file_columns,
  infer_schema = FALSE
)
```

3. Move the *saved_model* folder under */tmp*

4. Copy all the code from exercise 8.3 starting with step 2
```{r, eval = FALSE}
reload <- ml_load(sc, "saved_model")
reload
library(lubridate)
current <- tbl(sc, "flights") %>%
  filter(
    month == !! month(now()),
    dayofmonth == !! day(now())
  )
show_query(current)
head(current)
new_predictions <- ml_transform(
  x = reload,
  dataset = current
)
new_predictions %>%
  summarise(late_fligths = sum(prediction, na.rm = TRUE))
```

5. Change the `ml_load()` location to `"/tmp/saved_model"`

6. Close the Spark session
```{r}
spark_disconnect(sc)
```

7. `knit` the document to confirm it works

8. Click on File and then Publish

9. Select *Publish just this document*

10. Click *Publish anyway* on the warning

11. In RStudio Connect, select `Schedule`

12. Click on `Schedule output for default`

13. Click on `Run every weekday (Monday to Friday)`

14. Click Save


### 11.4 - Scheduled re-fitting

1. Create a new **RMarkdown** document

2. Copy the code from the **Class catchup** section in Spark Pipeline, unit 8
```{r}
library(tidyverse)
library(sparklyr)
library(lubridate)
top_rows <- read.csv("/usr/share/flights/data/flight_2008_1.csv", nrows = 5)
file_columns <- top_rows %>%
  rename_all(tolower) %>%
  map(function(x) "character")
conf <- spark_config()
conf$`sparklyr.cores.local` <- 4
conf$`sparklyr.shell.driver-memory` <- "8G"
conf$spark.memory.fraction <- 0.9
sc <- spark_connect(master = "local", config = conf, version = "2.0.0")
spark_flights <- spark_read_csv(
  sc,
  name = "flights",
  path = "/usr/share/flights/data/",
  memory = FALSE,
  columns = file_columns,
  infer_schema = FALSE
)
```

3. Move the *saved_pipeline* folder under */tmp*

4. Copy all the code from exercise 8.4 
```{r}
pipeline <- ml_load(sc, "/tmp/saved_pipeline")
pipeline
sample <- tbl(sc, "flights") %>%
  sample_frac(0.001) 
new_model <- ml_fit(pipeline, sample)
new_model
ml_save(new_model, "new_model", overwrite = TRUE)
list.files("new_model")
spark_disconnect(sc)
```

5. Change the `ml_load()` location to `"/tmp/saved_pipeline"`

8. `knit` the document to confirm it works

9. Click on File and then Publish

10. Select *Publish just this document*

11. Click *Publish anyway* on the warning

12. In RStudio Connect, select `Schedule`

13. Click on `Schedule output for default`

14. On the *Schedule Type* dropdown, select *Monthly*

15. Click Save



---
# Appendix    
- These slides were created with `rmarkdown` and `xaringan`. To learn more about the `xaringan` package, check out Alision's slides from her rstudio::conf 2019 workshop https://arm.rbind.io/slides/xaringan.html
